A palestra da empresa datarisk foi ministrada pelo seu co-fundador Gustavo Bernardo, formado em matemática com mestrado em estatística. A empresa tem como clientes principalmente bancos e instituições financeiras com grande volumes de dados, atuando na mineração de dados para detectar fraudes e padrões sobre os clientes destas instituições. Apesar de normalmente se tratarem de grandes empresas conforme contou Gustavo o banco de dados destas empresas nem sempre são bem estruturados o que acaba elevando a dificuldade de manipulação dos mesmos.
Para realizar o processo de mineração são necessárias a realização de algumas etapas de preparação dos dados que conforme já mencionado pela professora em sala, é a etapa que consome cerca de 80% de todo o tempo gasto para realizar a mineração de dados. A primeira etapa elencada é a limpeza, através da qual serão tratados os casos de "outliers"(dados discrepantes) e "missings"(a ausência de dados) em determinados atributos. 
Logo após é realizada a seleção das variáveis que serão mantidas e que constituirão o grande volume de dados a ser analisado, nesta etapa existem alguns problemas a serem enfrentados, como por exemplo, que variáveis serão mantidas, redução da dimensão, etc. Selecionar as principais variáveis traz algumas vantagens, pois nesse processo são removidos atributos irrelevantes, atributos redundantes, a simplicidade é se torna mais presente e também a interpretabilidade, além disso, dependendo do modelo isto acarretará no aumento da acurácia do mesmo. A execução desta etapa é realizada através de algoritmos e modelos estatísticos, tendo como exemplos disso os algoritmos do tipo "Filter", que irão filtrar as variáveis com bases estatísticas, dentre os algoritmos citados foi explicado superficialmente o algoritmo mRMR("maximum relevance minimum redundancy"). Existem também os algoritmos "Wrapper", responsáveis por analisar um conjunto de variáveis e suas combinações visando um modelo ótimo, foi abordado superficialmente o algoritmo "Random Forest". E por fim os algoritmos do tipo "Embedded" que selecionam variáveis enquanto o modelo é treinado, no qual o algoritmo destacado foi o LASSO. É válido lembrar que podem ser usados mais de um algoritmo na seleção de variáveis para a criação do modelo.
Com a seleção realizada, o que resta é aplicar as técnicas de mineração de dados sobre os dados extraídos com o intuito de descobrir padrões interessantes neles. E com o resultado em mãos analisá-los e tomar decisões sobre possíveis mudança do modelo, validade do conhecimento obtido, etc.

Comentários pessoais:

Num geral a palestra conseguiu elucidar muito bem o passo a passo necessário para a mineração de dados. Porém as explicações sobre os algoritmos não precisavam ter se aprofundado tanto, pois por se tratar de uma palestra, talvez saber apenas a funcionalidade e o objetivo do algoritmo sem entrar em detalhes de funcionamento fosse mais interessante, deixando em aberto o caminho para quem tivesse interesse em aprofundar mais o conhecimento ir atrás por conta própria. Ao meu ver essa explicação dos algoritmos acabou tornando a palestra um pouco massante.